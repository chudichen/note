1. reduceByKey：

   虽然两个函数都能得出正确的结果，但reduceByKey更适合使用在大数据集上。这是因为Spark知道它可以在每个分区移动数据之前将输出数据与一个公用的key结合。

2. groupByKey：

   另一方面，当调用groupByKey时，所有的键值对都会被移动，在网络上传输这些数据非常没有必要，因此避免使用groupByKey。

   为了确定将数据移动到哪一台主机上，Spark会对数据里的key调用一个分区算法。当移动数据大于计算机器的总内存时，spark会将数据写入到磁盘上。不过在保存时每次会处理一个key的数据，所以在单个key中键值大小超过内存容量时会存在内存溢出的异常。这将会在之后的Spark版本中优雅的处理，但是还是要注意避免写入到磁盘，这会严重影响性能。

3. 结论

   你可以想象在面对一个非常大的数据量时，reduceByKey和groupByKey之间的差距会非常大。